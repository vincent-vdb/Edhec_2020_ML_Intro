{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parameters and hyperparameters\n",
    "- hyperparameters optimization: the valid dataset\n",
    "- Cross validation\n",
    "- Gridsearch and randomsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyperparameters & Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For the moment, we learnt how to handle most of the steps of a machine learning project:\n",
    "- data visualization\n",
    "- data preparation\n",
    "- model training and regularization\n",
    "\n",
    "But how to choose carefully, and optimally, the regularization parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# I. Parameters vs hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In machine learning, we make a difference between parameters and hyperparameters:\n",
    "- parameters are learned by the model while fitting: we can't modify them directly\n",
    "- hyperparameters can be tuned: we can modify them directly\n",
    "\n",
    "> Optimizing hyperparameters may sometimes lead to sensitive performance improvements\n",
    "\n",
    "Up to now, we spoke almost only about parameters, without distinction, for pedagogical reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Any example of hyperparameters you already know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Examples of hyperparameters:\n",
    "- learning rate alpha\n",
    "- number of iterations of gradient descent\n",
    "- regularization type: L1 or L2\n",
    "- regularization value: `C` in logistic regression in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are several ways to optimize hyperparameters, but this is always the same idea at the end: **finding the set of hyperparameters that maximize a given metric**.\n",
    "\n",
    "How would you do that using train and test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here are the possibilities:\n",
    "1. training model and optimizing hyperparameters on train set\n",
    "2. training model on train set, optimizing hyperparameters on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What are the limits of such approaches?\n",
    "1. We train model and optimize on the same dataset, this might lead to several overfitting\n",
    "2. We optimize on the test dataset, this will bias the final evaluation\n",
    "\n",
    "So what is the solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Introduce the **validation dataset**: the dataset used only for hyperparameter optimization.\n",
    "\n",
    "So that now the typical dataset splitting would be the following:\n",
    "- train set ~ 60% of the data: fitting the model\n",
    "- validation set ~ 20% of the data: tuning the hyperparameters\n",
    "- test set ~ 20% of the data: evaluate the performance of the model\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ahZYvfiqQumVw-z0FkDVue7Pt9krAGCk\" width=\"400px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# II. Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In practice, most people do not use this kind of static splitting between train and valid sets: they use the cross-validation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.1. The cross-validation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **cross-validation** technique allows you to both train your model and tune hyperparameters on all your labeled data available, except the test set (of course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The process of the cross-validation is the following:\n",
    "- You split your data into a training set and a test set (e.g. 80%-20%)\n",
    "- You split your training set into cross-validation folds (for example `k=10` folds)\n",
    "- You train on all folds but one - and you do that `k` times, keeping each time a different fold aside\n",
    "- For each training you compute your evaluation metric\n",
    "- The error is then averaged over the `k` folds and is called the **cross-validation error**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=18iNqOegmMWxGkGbVD2rYPzsH4D97jRNC\" width=\"700px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Finally, once you find the best set of hyperparameters, you train your model on the entire train set, using those hyperparameters\n",
    "\n",
    "> Using cross-validation, you can expect to have a more robust model evaluation: you optimize your model not only on a subset of the train set, but on the whole train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.2. Stratified cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For classification issues, it might happen that you classes are imbalanced.\n",
    "\n",
    "Meaning, at least one class is overrepresented compared to the rest.\n",
    "\n",
    "The consequence of this is that the splitting may bias the training or the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's consider an example of binary classification (red or blue), with the following balance:\n",
    "- 1/3 blue\n",
    "- 2/3 red\n",
    "\n",
    "Now let's consider a cross validation on 9 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cross_val.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As one can see, this kind of cross validation might not be ideal.\n",
    "\n",
    "> the training and evaluation on each fold might be biased: sometimes blue is overrepresented, sometimes it's red\n",
    "\n",
    "So the solution is the stratified cross validation, keeping the same balance of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/stratified_cross_val.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to optimize the model, this cross validation technique will be used, **together with a hyperparameter research** method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# III. Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's look at the logistic regression signature in scikit-learn and check the hyperparameters:\n",
    "```python\n",
    "class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have for example the following hyperparameters:\n",
    "- `penalty` is the regularization type: it can be `'l1'`, `'l2'`, `'elasticnet'`, `'none'`\n",
    "- `C` is the regularization factor: the higher, the less regularized e.g. `0.01`, `0.1`, `1.`\n",
    "- `max_iter` it the maximum number of iterations: e.g. `10`, `100`, `1000`\n",
    "\n",
    "How would you test all those values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are two main ways of optimizing those hyperparameters:\n",
    "- Grid Search\n",
    "- Random Search\n",
    "\n",
    "Let's have a look at them, and use them with scikit-learn.\n",
    "\n",
    "But first, let's reuse the Titanic survival prediction task to apply those methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Title_Miss.</th>\n",
       "      <th>Title_Mr.</th>\n",
       "      <th>Title_Mrs.</th>\n",
       "      <th>Title_Others</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.824744</td>\n",
       "      <td>-0.589288</td>\n",
       "      <td>-0.499958</td>\n",
       "      <td>0.431108</td>\n",
       "      <td>-0.474059</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.571327</td>\n",
       "      <td>0.644485</td>\n",
       "      <td>0.788503</td>\n",
       "      <td>0.431108</td>\n",
       "      <td>-0.474059</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.824744</td>\n",
       "      <td>-0.280845</td>\n",
       "      <td>-0.486376</td>\n",
       "      <td>-0.474932</td>\n",
       "      <td>-0.474059</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.571327</td>\n",
       "      <td>0.413153</td>\n",
       "      <td>0.422623</td>\n",
       "      <td>0.431108</td>\n",
       "      <td>-0.474059</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.824744</td>\n",
       "      <td>0.413153</td>\n",
       "      <td>-0.483861</td>\n",
       "      <td>-0.474932</td>\n",
       "      <td>-0.474059</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Pclass       Age      Fare     SibSp     Parch Sex  Embarked_Q  \\\n",
       "PassengerId                                                                     \n",
       "1            0.824744 -0.589288 -0.499958  0.431108 -0.474059   0           0   \n",
       "2           -1.571327  0.644485  0.788503  0.431108 -0.474059   1           0   \n",
       "3            0.824744 -0.280845 -0.486376 -0.474932 -0.474059   1           0   \n",
       "4           -1.571327  0.413153  0.422623  0.431108 -0.474059   1           0   \n",
       "5            0.824744  0.413153 -0.483861 -0.474932 -0.474059   0           0   \n",
       "\n",
       "             Embarked_S  Title_Miss.  Title_Mr.  Title_Mrs.  Title_Others  \n",
       "PassengerId                                                                \n",
       "1                     1            0          1           0             0  \n",
       "2                     0            0          0           1             0  \n",
       "3                     1            1          0           0             0  \n",
       "4                     1            0          0           1             0  \n",
       "5                     1            0          1           0             0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# Import the data\n",
    "with open('titanic_cleaned.pkl', 'rb') as file:\n",
    "    X, y = pickle.load(file)\n",
    "    \n",
    "# Split it\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## III.1. `GridSearchCV`: Explicit Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The grid search of scikit-learn `sklearn.model_selection.GridSearchCV` will test, for each provided combination of hyperparameters, an evaluation using cross validation.\n",
    "\n",
    "GridSearchCV stands for Grid Search Cross Validation.\n",
    "\n",
    "Why grid? Because all the hyperparameters combinations are tested, just like trying all the points in a grid of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How to implement it?\n",
    "\n",
    "That's pretty easy thanks to scikit-learn. The signature of the `GridSearchCV` is the following:\n",
    "```python\n",
    "class sklearn.model_selection.GridSearchCV(estimator, param_grid, *, scoring=None, n_jobs=None, iid='deprecated', refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
    "```\n",
    "\n",
    "with:\n",
    "- `estimator`: a model instance, e.g. `LogisticRegression()`\n",
    "- `param_grid`: a dict with all hyperparams and values to test\n",
    "- `scoring`: the metric to maximize, e.g. `'accuracy'`\n",
    "- `cv`: the number of cross validation folds, e.g. `5`\n",
    "\n",
    "Let's implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T21:22:11.898398Z",
     "start_time": "2020-01-29T21:22:11.473379Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'C': [0.01, 0.1, 1.0], 'max_iter': [10, 100, 1000],\n",
       "                         'penalty': ['l2', 'none']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters we want to test\n",
    "param_grid = {'penalty': ['l2', 'none'], \n",
    "              'C': [0.01, 0.1, 1.],\n",
    "              'max_iter': [10, 100, 1000]}\n",
    "# Define the gridsearch object\n",
    "grid = GridSearchCV(LogisticRegression(),\n",
    "                    param_grid,\n",
    "                    scoring='accuracy',\n",
    "                    cv=5\n",
    "                   )\n",
    "# Fit and wait\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once the fit finished, many informations are available:\n",
    "- all results in `.results_`: quite hard to read but useful if you need anything\n",
    "- optimal set of hyperparameters in `.best_params_`\n",
    "- best score in `.best_score_`\n",
    "- best estimator in `best_estimator_` if you need to retrieve the object itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T21:22:19.593374Z",
     "start_time": "2020-01-29T21:22:19.585809Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0, 'max_iter': 100, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The best params here\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T21:22:22.609319Z",
     "start_time": "2020-01-29T21:22:22.602852Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8143405889884763"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The best score\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## III.2. `RandomizedSearchCV`: a randomized search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A grid search is quite explicit and straightforward: it tests all the specified values. But sometimes, this is not the most efficient approach.\n",
    "\n",
    "What if we test `C` in `0.1`, `1` but the optimal value appears to be more close to `0.7`? With grid search we would never know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A randomized search can tackle this issue: ask to test, randomly, values between `0.1` and `1`.\n",
    "\n",
    "It might find out a best value to be `0.6`, or `0.85`, not the absolute optimal, but better than `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How to implement it? Same as a grid search. The only difference will be in giving not a list of values, but a **distribution** of values, and a **number of combinations of hyperparameters** to test.\n",
    "\n",
    "To do so, we will use `scipy` library. Let's have an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score=nan,\n",
       "                   estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                dual=False, fit_intercept=True,\n",
       "                                                intercept_scaling=1,\n",
       "                                                l1_ratio=None, max_iter=100,\n",
       "                                                multi_class='auto', n_jobs=None,\n",
       "                                                penalty='l2', random_state=None,\n",
       "                                                solver='lbfgs', tol=0.0001,\n",
       "                                                verbose=0, warm_start=False),\n",
       "                   iid='deprecated', n_iter=50, n_jobs=None,\n",
       "                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fc69d53d5d0>,\n",
       "                                        'max_iter': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fc69d0ace10>,\n",
       "                                        'penalty': ['l2', 'none']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=0, refit=True,\n",
       "                   return_train_score=False, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy\n",
    "\n",
    "param_dist = {\n",
    "    'penalty': ['l2', 'none'],\n",
    "    'C': scipy.stats.expon(scale=0.5),\n",
    "    'max_iter': scipy.stats.expon(scale=500) \n",
    "    }\n",
    "\n",
    "n_iter_search = 50 # The number of set of hyperparameters to try out\n",
    "randsearch = RandomizedSearchCV(LogisticRegression(),\n",
    "                                param_distributions=param_dist,\n",
    "                                n_iter=n_iter_search,\n",
    "                                scoring='accuracy',\n",
    "                                cv=5,\n",
    "                                random_state=0\n",
    "                               )\n",
    "\n",
    "randsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then you can have all the attributes you had with a `GridSearchCV`:\n",
    "- `cv_results_`\n",
    "- `best_params_`\n",
    "- `best_score_`\n",
    "- `best_estimator_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params are: {'C': 1.575085018411753, 'max_iter': 75.61542857136058, 'penalty': 'l2'}\n",
      "best score is: 0.8171574903969271\n"
     ]
    }
   ],
   "source": [
    "print('best params are:', randsearch.best_params_)\n",
    "print('best score is:', randsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III.3. `GridSearchCV` vs `RandomizedSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, people are more likely to choose to work with a GridSearch rather than a RandomizedSearch, probably because it seems less \"scary\" : humans do not like random things.\n",
    "\n",
    "Most people use grid search most of the time. Random search is quite powerful though, even if less intuitive.\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1C2bh--ETFTMS9d3TPl2c_WlmFtQPviUv\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
