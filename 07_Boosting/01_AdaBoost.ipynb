{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging vs boosting\n",
    "- Stumps and adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this lecture, we will talk about another type of ensemble learning: the boosting.\n",
    "\n",
    "More specifically, we will talk about **adaboost** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# I. Reminder : Decision Trees and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## I.1. Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As a reminder, in a Decision Tree, we perform binary classifications and can grow the depth of a tree to make the resulting decision boundary complex enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1GxvuU-RU8afLzYZc2eegZnRtyUzoL4AF\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To build the tree, we choose each time the feature that splits our data the best way possible, thanks to a disorder measurement:\n",
    "- Gini impurity or Entropy for classification\n",
    "- Mean squared error for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that the Gini impurity can be defined as follows:\n",
    "$$\n",
    "G(f) = 1 - \\sum {f_i}^2\n",
    "$$\n",
    "\n",
    "Where $f_i$ is the proportion of element $i$ in the node/leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I.2. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In a random frest, we build an ensemble of decision drees in several key steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 1**: Boostrap subsampling\n",
    "\n",
    "Pick *n* data points randomly.\n",
    "\n",
    "We are allowed to pick the same point more than 1 time for each bootstrap sample, since we pick with replacement. We will build several bootstrap subsamples, say *K*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Cmh0STP2ZmfEnfe-BXuMLOjFgTeUKZDu\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 2**: Decision trees training\n",
    "\n",
    "Then, we train *K* decision trees, one for each bootstrap subsample, and use only a subspace of the features each time. \n",
    "\n",
    "This builds a wide variety of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=10bRgs3x9O4CoGXhQ0r_teSGzXPq15-2C\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 3**: Aggregating with majority vote\n",
    "\n",
    "Finally, in order to predict a new sample, we run it through all the decision trees, and apply a majority vote on the output of each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We first **bootstrapped** the data, and then **aggregated** the results. This is called Bagging, and stands for Bootstrap Aggregating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# II. Limits of Bagging: same region same error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's consider a binary classification problem: 0 or 1.\n",
    "\n",
    " Let’s consider 3 decision trees $h_1$, $h_2$ and $h_3$, which produce a classification result and can be either right or wrong.\n",
    "\n",
    "The final prediction will be the majority vote $H_T$ of those three decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we plot the results of the 3 classifiers, there are regions in which the classifiers will be wrong. These regions are represented in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1iF2nzHrwnMTXw7IDOh-l_UHvC80aF9jK\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This example works perfectly well.\n",
    "\n",
    "As long as only one classifier is wrong at a time, the two others are correct. Thus the final prediction is right, and thus our random forest works perfectly.\n",
    "\n",
    "\n",
    "But what if more than 1 decision tree out of 3 is wrong on a given region?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T17:31:41.943476Z",
     "start_time": "2019-02-08T17:31:41.937887Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1AXptTl3qP2HmaScqFpkXXME_V0dutUP9\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This leads to the following limitations and potential solutions:\n",
    "- instead of training parallel decision trees, we need to **train models sequentially**\n",
    "- in order avoir the \"same region, same error\", each decision tree should focus on **correcting the previous tree errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# III. Adaptative Boosting: AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## III.1. AdaBoost vs. Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are several changes between AdaBoost and Random Forests:\n",
    "- decision trees have a very limited max depth of 1: they are thus called **stumps**\n",
    "- each stump may now have an associated weight in the final prediction\n",
    "- decisions trees are no longer independant: each stump will be trained depending on the previous one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The decision trees have now a limited depth of 1. They are called **stumps**, and lead to a simple binary classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1VwcO6eE83eHLzDIDGy8ncdLqUrny2Gq2\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The stumps are what we call **weak learners**. Weak learners are algorithms whose error rate is slightly under 50% as illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=17UvX3obE1yj47eAdkZzrZaw7uw_6uLVu\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The forest of stumps allows for different weights on each stump: some stumps may have a higher weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1hka_fKK0fEA1VyWIfsiTmdmtp3a8kLRB\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.2. Step by step algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Boosting trains sequentially a series of low performing decision trees (weak learners), by adjusting the error metric over time.\n",
    "\n",
    "Let's say we have 2 features $x_1$ and $x_2$ and we want to predict $y$.\n",
    "\n",
    "We will see step by step how adaboost works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 1**: Initialize the weight given to each sample\n",
    "\n",
    "When we start our AdaBoost algorithm, we should assign a weight $\\frac {1}{n}$ to each of the $n$ samples.\n",
    "\n",
    "Indeed, in adaboost, not only decisions trees have a weight, samples too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ECDQCid0Qj_3IPwuXU5h193Yf2dhynH5\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 2**: Find the best stump\n",
    "\n",
    "After initializing weights, the aim is to find the best stump, i.e the feature split that minimize the Gini impurity.\n",
    "\n",
    "For example, here, we would typically split along the $x_1$ variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1otRVrfmOubEzB4N5NRzmQl1NASJHGWtq\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 3**: Assign a weight to this stump\n",
    "\n",
    "As said before, that **each stump has a different weight** in the final vote.\n",
    "\n",
    "This stump weight has to take into account the accuracy of the stump: the more the stump is right, the larger the weight.\n",
    "\n",
    "So the weight will depend on the classification performance of each stump. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our example, the stump misclassified 1 sample out of 12. \n",
    "\n",
    "First, we need to compute the total error called ${\\epsilon_{t}}$ of the **stump $t$**:\n",
    "$$\n",
    "{\\epsilon_{t}} = \\frac {2}{12}\n",
    "$$. \n",
    "\n",
    "Then, we compute the **amount of say** $\\alpha_t$, i.e the weight of the stump in the final vote:\n",
    "\n",
    "$$ \\alpha_t = \\frac {1} {2} ln \\frac {1-\\epsilon_{t}} {\\epsilon_{t}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T12:34:04.706440Z",
     "start_time": "2019-04-17T12:34:04.668185Z"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1V2Bo7c6M2rUEs0-6khO452X7gqFrNlcy\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see some extreme cases:\n",
    "- if the stump is not better than a random classifier with an accuracy of 50%, its amount of say is 0\n",
    "- if the total error is low, the amount of say is large\n",
    "- on the opposite, a large total error leads to a large, **negative** amount of say"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 4**: Update the weights of incorrectly classified samples\n",
    "\n",
    "Now, we need to **update the weights of misclassified samples**.\n",
    "\n",
    "By increasing their weight, the next stump will be more likely to correct those misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To compute the weights $w_{t+1} (i)$ of the sample $i$ for the stump $t+1$, we use again the amount of say:\n",
    "\n",
    "$$\n",
    "\\large w_{t+1}(i) = w_{t}(i) e^{\\alpha_t } \n",
    "$$\n",
    "    \n",
    "Let's have a look at the function $e^{\\text{amount of say}}$ to understand:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1cYmpoHcW771OwP7WqQDv1PpCdhn1nDae\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Doing this update, if the amount of say is large, the weight on misclassified samples will be large.\n",
    "\n",
    "Another way of saying it: \n",
    "\n",
    "> if a classifier was performing very well except for one sample, this one sample will have a really large weight for next stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 5**: Update the weights of correctly classified samples\n",
    "\n",
    "The same way, we need to **update the weights of the well classified samples**.\n",
    "\n",
    "This is the same idea as before, in the other way around:\n",
    "\n",
    "$$ \n",
    "\\large w_{t+1}(i) = { w_{t}(i) } e ^{ - \\alpha_t } \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1jcghhM5RXLevq2zQCy-SO5K0odmrqiLh\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> If the classifier is good, thus having a large amount of say, we will lower the weight of the well classified samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 6**: Normalize the weights to 1\n",
    "\n",
    "Now the weights of the samples do not sum to 1. We just need to normalize the weights to ensure a sum of 1:\n",
    "\n",
    "$$\n",
    "w_{t+1}(i) = \\frac {w_{t+1}(i)} {\\sum_i {w_{t+1}(i)}}\n",
    "$$\n",
    "\n",
    "We end up with a new dataset looking like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1TQmipTp6cpMk2e1n-1HkT4GwrqstPO3W\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 7**: Rebalance the samples\n",
    "\n",
    "Using the updated sample weights, we can go to the next iteration and fit the next stump.\n",
    "\n",
    "There are 2 ways to handle the new imbalanced weights:\n",
    "- apply weighted Gini \n",
    "- create duplicates of the observations by considering the weights as a distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to create duplicates with the right weights as distribution, we compute the cumulative sum of the weights, pick randomly a number between 0 and 1, and select the corresponding observation.\n",
    "\n",
    "We do this until the new collection is the same size as the original.\n",
    "\n",
    "A drawback is that there will be missing samples from the original dataset doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1-GmuTb_gbkl2aUr8aX00n9EPBqSG4h0S\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 8**: Compute the next stump\n",
    "\n",
    "The next stump will then have to classify very carefully the large weight samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1X_R3IQjypKHJ3VgnBFSZ947mLdBvJrLh\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 9**: Iterate and build forest of stumps...\n",
    "\n",
    "Iterate this process until the pre-defined hyperparameter of number of iterations (`n_iter`) has been reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Step 10**: Once trained, make classification with the forest of stumps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We just have look at the amount of say and prediction of each stump:\n",
    "- we sum the amount of say $\\alpha_t$ of the stumps that predict the new observation as class 1\n",
    "- we sum the amount of say $\\alpha_t$ of the stumps that classify the new observation as class 0\n",
    "\n",
    "And finally predict the class that has the largest summed amount of say."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1FBuu8-rJifE3ubbYGQRFte64nvpA4C0N\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### To summarize:\n",
    "\n",
    "- Train the stump on the samples → $h_1$\n",
    "- Train the stump with overweighted data on the regions in which $h_1$ performs poorly → $h_2$\n",
    "- Train the stump with overweighted data on the regions in which $h_1$ ≠ $h_2$ → $h_3$\n",
    "- ...\n",
    "\n",
    "Instead of training the decision trees in **parallel** (as in bagging), we can train them **sequentially**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1zxEJ_AoRCV_lwvLitK0stNSsnJa5ko7_\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### II.3. [Optional] Pseudo-Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's wrap up in a small pseudo-code what we covered so far.\n",
    "\n",
    "*Step 1* : Let $w_{t}(i) = \\frac {1} {N}$ where $N$ denotes the number of training samples, and let $T$ be the chosen number of iterations.\n",
    "\n",
    "*Step 2* : For $t$ in $T$ :\n",
    "\n",
    "  a. Pick $h^t$ the weak classifier that minimizes $\\epsilon_{t}$ , here we implement a weighted error metric :\n",
    "  \n",
    "  $$ \\epsilon_{t} = \\sum _{i=1}^{m} w_{t}(i)[y_{i}\\neq h(x_{i})] $$\n",
    "\n",
    "  b. Compute the weight of the stump :\n",
    "  \n",
    "  $$ \\alpha_t = \\frac {1} {2} ln \\frac {1-\\epsilon_{t}} {\\epsilon_{t}} $$\n",
    " \n",
    " c. Update the weights of the training examples $w_{t+1}^{i}$ :\n",
    " \n",
    " $$ w_{t+1}(i) = \\frac { w_{t}(i) } { Z } e ^{- \\alpha^t h^t(x) y(x)} $$\n",
    " \n",
    " And go back to step a).\n",
    "\n",
    "*Step 3* : $$ H(x) = sign(\\alpha^1 h^1(x) + \\alpha^2 h^2(x) + ... + \\alpha^T h^T(x)) $$\n",
    "\n",
    "And we're done ! This algorithm is called **AdaBoost**. This is the most important algorithm one needs to understand in order to fully understand Gradient Boosting we'll introduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "AdaBoost has for a long time been considered as one of the few algorithms that does not overfit. But lately, it has been proven to overfit at some point, and one should be aware of it. AdaBoost is vastly used in face detection to assess whether there is a face in the video or not. AdaBoost can also be used as a regression algorithm.\n",
    "\n",
    "Adaboost can be generalized to regression by simply taking the average output among the forest of stumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
