{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this section we will speak about model evaluation.\n",
    "\n",
    "Indeed, a good, robust, unbiased model evaluation is key for good performances.\n",
    "\n",
    "What is important is a good dataset split, and to choose the right evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reminder about ML project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On that typical timeline, when would you define your evaluation method:\n",
    "- defining the problem\n",
    "- collecting data\n",
    "- cleaning/preparing data\n",
    "- training and optimizing model\n",
    "- model evaluation\n",
    "- model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The evaluation method, that commonly implies at least a **metric**, has to be defined at the **beginning of a project**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# I. Training & Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is crucial to separate the data into two sets:\n",
    "- data for **training** a model: the **train set**\n",
    "- data for **evaluating**, or testing your model: the **test set**\n",
    "\n",
    "Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A student is expected to train on the courses and exercises, but to be evaluated on a completely new, somehow related, test.\n",
    "\n",
    "If the student knows all the answers to the exercises, it does not necessarily mean he will be able to generalize well on the test.\n",
    "\n",
    "This is the same for a model.\n",
    "\n",
    "You need to test your model on **new data** to make sure he learnt correcly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1x4j8bjbFbkSUKKsRPcPAGeJs8JF1OI9g\" width=\"350\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The usual convention is the following:\n",
    "- training set: 80 % of the data\n",
    "- test set : 20 % of the data\n",
    "\n",
    "> The idea is that you keep most of the data to train your model correctly, and evaluate it on a smaller, but statistically enough, dataset\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1I0Dn2d0RUM1BrOGG40VKwJHfKE-Lh6VE\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoir any bias, it is recommended to shuffle the data before the split.\n",
    "\n",
    "Meaning for example, over 100 samples in the original dataset:\n",
    "- 80 random samples will be in the train set\n",
    "- 20 remaining samples will be in the test set\n",
    "\n",
    "This way we ensure there is no bias: like for example a sorted dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To recap, we have:\n",
    "\n",
    "-  a **Training set**: A set of samples used for **learning**. It **fits the parameters (or weights) of the model**.\n",
    "   \n",
    "    Usually noted `X_train` for the features and `y_train` for the associated labels.\n",
    "\n",
    "\n",
    "- a **Test set**: A set of new, unseen samples used only to **evaluate** of the model.\n",
    "\n",
    "    Usually noted `X_test` for the features and `y_test` for the associated targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn provides a function to split your data into train set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now imagine having what we call an imbalanced dataset: most of the samples belong to the same class, like the following example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/empty_split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we let the randomness choose for us, we might end up with a split like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/bad_split_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is wrong, how could we have a proper evaluation with only one class in test set?\n",
    "\n",
    "Another risk (less likely), is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/bad_split_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Anyway, the point is, we want to **keep the class balance** between our datasets.\n",
    "\n",
    "This is exactly what **stratification** does: it ensures the class balance remains the same:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/stratified_split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This can be done easily thanks to scikit-learn, with the parameter `stratify` of the `train_test_split` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Sklearn provides a function to split your data into train set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# II. Evaluation for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's consider again our customer churn classification model: \n",
    "- 0 for not churner\n",
    "- 1 for churner\n",
    "\n",
    "We evaluated our classification model by computing the **accuracy**: the number of correct predictions over the total number of predictions.\n",
    "\n",
    "In other words, it's the **percentage of correct predictions** of the model, on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Though, is it always a good metric for evaluating a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Confusion matrix, Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let the results of our customer be the following on the test set:\n",
    "\n",
    "| | Not Churner | Churner | \n",
    "|------|------|------|\n",
    "| Predicted Not Churner üëé | 49 | 5¬†|\n",
    "| Predict Churner üëç | 1 | 45 |\n",
    "\n",
    "This table is called a **confusion matrix**.\n",
    "\n",
    "We call:\n",
    "* True Positive (TP): the number of predicted positive that are indeed positive - here 45\n",
    "* True Negative (TN): the number of predicted negative that are indeed negative - here 49\n",
    "* False Positive (FP): the number of predicted positive that are in actually negative - here 1\n",
    "* False Negative (FN): the number of predicted negative that are in actually positive - here 5\n",
    "\n",
    "> What is the accuracy here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "| | Not Churner | Churner | \n",
    "|------|------|------|\n",
    "| Predicted Not Churner üëé | 49 | 5 |\n",
    "| Predict Churner üëç | 1 | 45 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Accuracy as we know it can be defined as:\n",
    "$$Accuracy = \\frac{TP + TN}{TP + TN + FN + FP}$$\n",
    "\n",
    "In our case, $Accuracy = \\frac{49 + 45}{49 + 5 + 1 45} = 0.94$ \n",
    "\n",
    "Our model has an accuracy of 94%. For this kind of application, this would be quite acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now our problem is not anymore a customer churn, but a cancer detection.\n",
    "\n",
    "| | Healthy | Cancerous | \n",
    "|------|------|------|\n",
    "| Predicted Healthy üëé | 49 | 5 |\n",
    "| Predict Cancerous üëç | 1 | 45 |\n",
    "\n",
    "Can we afford to let those 5 False Negative die?\n",
    "\n",
    "Is accuracy a good metric for such a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Precision & Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There exist two metrics really useful for such problems:\n",
    "\n",
    "- **Precision**: How many selected items are relevant?\n",
    "\n",
    "Here, how many cancerous classified as such are indeed cancerous?\n",
    "\n",
    "It's 45/46 = 0.978: 97.8% of cancerous classified patients actually are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Recall**: How many relevant items are selected?\n",
    "\n",
    "Here, how many actually cancerous patients are classified as such? \n",
    "\n",
    "It's 45/50 = 90:  90% of cancerous patients are actually detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The formulas are the following:\n",
    "\n",
    "$${Precision = \\frac{TP}{TP + FP}}$$\n",
    "\n",
    "$${Recall = \\frac{TP}{P} = \\frac{TP}{TP + FN}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1LM2d2k6lPt8g3skTW3MTOErXwORhndYB\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we want to maximize or compute both metrics, there exist the F1-score. Which is the harmonic mean of Precision and Recall:\n",
    "\n",
    "$$\n",
    "{F1\\_score = \\frac{2 \\times (Precision \\times Recall)}{Precision + Recall}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here, we would have a F1-score of:\n",
    "$$\n",
    "F1\\_score = \\frac{2 \\times 0.978 * 0.9)}{0.978 + 0.9} = 0.9375 = 93.75 \\%\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> NB: We can define Precision, Recall and F1-score per class. Here we assumed it was for the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Most of the time, a classifier returns a value between 0 and 1 (that usually considered as a probability), and we consider the highest probability as the predicted class.\n",
    "\n",
    "> In scikit-learn instead of predicting the class label with `model.predict(X)`,\n",
    "one can do `model.predict_proba(X)` to retrieve probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For each value of the threshold, one can compute the **False Positive Rate (FPR)** and the **True Positive Rate (TPR)**.\n",
    "\n",
    "Then we can plot the curve ROC with the TPR as a function of the FPR:\n",
    "\n",
    "![](images/roc_curves.png)\n",
    "\n",
    "> Nice visualization of the ROC curve and impact of data distribution/threshold on it: http://www.navan.name/roc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To summarize about ROC curves:\n",
    "- A perfect model's ROC curve will stick to the roof of the plot\n",
    "- A random model's ROC curve will be a y=x curve\n",
    "- The better the model, the higher the curve\n",
    "\n",
    "Can we get a metric out of this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **ROC Area Under Curve** (ROC AUC) can be seen as a metric:\n",
    "- ROC AUC = 1 for a perfect classifier\n",
    "- ROC AUC = 0.5 for a random classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# III. Evaluation for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Regression works just like logistic regression.\n",
    "\n",
    "We have a set of features X, labels y and a set of parameters $\\theta$ such that:\n",
    "$$\n",
    "\\hat{y} = h_\\theta(x) = \\sum \\theta_j X_j + b\n",
    "$$\n",
    "\n",
    "Where $\\hat{y}$ is the prediction of the label y.\n",
    "\n",
    "> In a regression, the label y is a quantitative data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training method is the same as a logistic regression: gradient descent.\n",
    "\n",
    "But the loss is different, this is usually the Mean Squared Error:\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\frac{1}{2m} \\sum_i^m (y^{(i)} - \\hat{y}^{(i)})^2 = \\frac{1}{2m} \\sum_i^m (y^{(i)} - h_\\theta(x^{(i)}))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is an example of visualization of the evolution of a linear regression with gradient descent:\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1_l5TDRJMBhHY-_GNehBD4d27aElKQJq1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Residual Sum of Squares "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We trained the Regression model by minimizing the distance between the model predictions and the data points\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1UmfQle9OImxlTgu3AEUsk9Yvo8RYaVsR\" width=\"100%\">\n",
    "\n",
    "\n",
    "If we sum all the squares of distances (in order to sum only positive values), we obtain the **Residual Sum of Squares**:\n",
    "\n",
    "$$\n",
    "RSS=\\sum_{i=1}^{m}(y^{(i)}-h_\\theta(x^{(i)}))^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In practice, this metric is not very commonly used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In order to have a mean error, we can:\n",
    "\n",
    "- Divide the RSS by the number of points (i.e. averaging). This gives us the **Mean Squared Error** (MSE):\n",
    "\n",
    "$$\n",
    "MSE=\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}-h_\\theta(x^{(i)}))^{2}\n",
    "$$\n",
    "\n",
    "> This metric is very commonly used: a MSE of 0 means a perfect regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For more interpretability, one can compute the square of the MSE. Thus the metric is directly related to an interpretable value. This is called the **Root Mean Squared Error** (RMSE):\n",
    "\n",
    "$$\n",
    "RMSE=\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}-h_\\theta(x^{(i)}))^{2}}\n",
    "$$\n",
    "\n",
    "\n",
    "> For example, if your home price regression model predict a house value of 1200k‚Ç¨ while having a RMSE of 20k‚Ç¨, it means you can be pretty confident in your estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, a very commonly used metric in regression: $R^2$\n",
    "\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{m}(y^{(i)}-h_\\theta(x^{(i)}))^{2} }{\\sum_{i=1}^{m}(y^{(i)}-\\bar{y})^{2}}\n",
    "$$\n",
    "\n",
    "Where $\\hat{y}$ is the mean value of all the $y^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$R^2$ gives an indication of **the correlation between the true and the predicted values**:\n",
    "- if $R^2 \\simeq 1$: the predictions are highly correlated to the real data\n",
    "- if $R^2 \\simeq 0$: there is no correlation between the predictions and the real data\n",
    "- if $R^2 \\simeq -1$: there is a high, inverse correlation between the predictions and the real data\n",
    "\n",
    "> N.B.: Yes, the R2 score can be negative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üî¶ **Hint**: For understanding the trade-off of the threshold chosen, consider an airport security. Since passengers can be potential threats to safety, scanners may be set to trigger alarms on low-risk items like belt buckles and keys (**low specificity**) in order to increase the probability of identifying dangerous objects and minimize the risk of missing objects that do pose a threat (**high sensitivity**). \n",
    "\n",
    "\n",
    "As an example, let's build a ROC curve. We continue with our airport security example. Suppose our classifier returned the following scores (\"+\" represents a true threat, while a high score, close to 1, means a confident prediction that the observation is a threat) for 6 observations :\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1byt8YC6h9klPF2s0lUYZ4rDyo3gJ05A1\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Then we can compute the TPR (=TP/P) and FPR (=FP/P) for every threshold range: \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1hJxvUQKKS5RZzv6FvbKKH7Ua-nsawyDa\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "Which gives us the following ROC curve:\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1dXCxWe5jiEHz_bsf5wXziK8mqJjxEzXX\" width=\"300\">\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
