{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trees basics: node, leaf\n",
    "- Entropy and gini\n",
    "- Classification with trees\n",
    "- Limits of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing:\n",
    "\n",
    "- Decision tree implementation for classification, with decision function and display of the tree\n",
    "- decision tree for regression with short example of implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Today we will explore a new **classification** machine learning algorithm (supervised for categorical data) called **Decision Trees**. ðŸŒ³\n",
    "\n",
    "It is:\n",
    "- one of the oldest ML algorithm\n",
    "- extremely robust\n",
    "- quite intuitive\n",
    "\n",
    "We will also discover an extension of the Decision Trees: the **Random Forests** ðŸŒ³ðŸŒ³ðŸŒ³\n",
    "\n",
    "Let's dig in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# I. Trees Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## I.1. Trees and linearly separable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Trees can be efficient on non linearly separable data.\n",
    "\n",
    "Let's have a toy example: what would be the conditions to sell a lot of ice creams on the beach?\n",
    "\n",
    "- Warm temperature\n",
    "- Sunny weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can draw this dataset on a simple scatter plot:\n",
    "\n",
    "![](images/ice_cream_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Would you say this data is linearly separable? And why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Though not linearly separable, this data can be well classified with a sequence of two questions:\n",
    "- is the temperature warm?\n",
    "- is the weather sunny?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Another way to represent it is the following:\n",
    "\n",
    "![](images/ice_cream_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I.2. Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this tree, we can see three different objects:\n",
    "- the **root node** is the first node, here `warm?`\n",
    "- a **decision node** is just any node with two (or sometimes more) branches\n",
    "- a **leaf** is does not contain any split, it contains a final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I.3. An example of more complex tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1am_borVWERNA0BvOgsIy2HPqwJexAdD7\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# II. Decision trees for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.1. Greedy algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our tree is based on a so called **greedy algorithm**: meaning it only tries to optimize one step at a time.\n",
    "\n",
    "In our case, it means our tree is not optimized globally, but one node at a time.\n",
    "\n",
    "This can be seen as a recursive algorithm:\n",
    "1. take all samples in a node\n",
    "2. find a threshold in a feature that minimizes the disorder\n",
    "3. split this into two new nodes\n",
    "4. go back to 1. until your node is pure, thus becoming a leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The only question here is: how do we measure disorder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.2. Disorder measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are several ways to measure disorder. We will here cover two of them, the most used in decision trees:\n",
    "- Entropy\n",
    "- Gini impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One of the most common ways of measuring disorder is using **entropy**.\n",
    "\n",
    "Entropy is a generic word, used in various fields like physics (thermodynamics) or computer science.\n",
    "\n",
    "It always refers to a sort of disorder, and can be expressed as follow:\n",
    "$$\n",
    "E = -\\sum p_i log_2 p_i\n",
    "$$\n",
    "\n",
    "Where $p_i$ is defined as the proportion of element $i$ in a subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's consider the following example:\n",
    "\n",
    "![](images/root_node.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The entropy would be:\n",
    "$$\n",
    "E = - \\frac{3}{10} log_2 \\frac{3}{10} - \\frac{7}{10} log_2 \\frac{7}{10} \\simeq 0.88\n",
    "$$\n",
    "\n",
    "Because we have:\n",
    "- $p_{blue} = \\frac{3}{10}$, since we have 3 blue balls out of 10\n",
    "- $p_{red} = \\frac{7}{10}$, since we have 7 blue balls out of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This entropy has several interesting properties for disorder measurement:\n",
    "- if $p_{blue} = 0$, then $p_{red} = 1$ and $ E = 0 $\n",
    "- if $p_{blue} = p_{red} = 0.5$, then $ E = 1$\n",
    "\n",
    "![](images/Entropy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gini impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Gini impurity is also a disorder measurement. The formula is the following:\n",
    "$$\n",
    "G(f)=\\sum f_{i}(1-f_{i})=\\sum (f_{i}-{f_{i}}^{2})=\\sum f_{i}-\\sum {f_{i}}^{2}=1-\\sum {f_{i}}^{{2}}\n",
    "$$\n",
    "\n",
    "Where this time the proportion of element $i$ is defined by $f_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On our same example:\n",
    "\n",
    "![](images/root_node.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The gini impurity would be:\n",
    "$$\n",
    "G = 1 - \\left(\\frac{3}{10}\\right)^2 - \\left(\\frac{7}{10}\\right)^2 = 0.42\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-0.5*0.5-0.5*0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once again we have our extreme cases:\n",
    "- if $f_{blue} = 0$, then $f_{red} = 1$ and $ E = 0 $\n",
    "- if $f_{blue} = f_{red} = 0.5$, then $ E = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.3. Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, in order to split a node optimally, we need to compute the a loss to minimize is the measure of the **disorder decrease** thanks to our split.\n",
    "\n",
    "To compute this loss, we consider the two children nodes, that we will call *left* and *right*.\n",
    "\n",
    "The formula is the following:\n",
    "$$\n",
    "\\mathcal{L} = \\frac{m_{left}}{m} G_{left} + \\frac{m_{right}}{m}G_{right}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $m$, $m_{left}$ and $m_{right}$ are the number of samples in each nodes respectively\n",
    "- $G_{left}$ and $G_{right}$ are the *Gini impurities* of *left* and *right* nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Of course, one can use entropy instead of gini impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Imagine we choose a split decision, we then have a **parent node** and **two children nodes**:\n",
    "![](images/split_nodes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus here the loss $\\mathcal{L}$ would be:\n",
    "$$\n",
    "\\mathcal{L} = \\frac{3}{10}0.44 + \\frac{7}{10} 0.41 = 0.42\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The algorithm will find the split that minimizes this entropy to create the new nodes.\n",
    "\n",
    "Then, until we reach a limit (pure leaves or any given constraint), it will recursively keep splitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.4. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are available in scikit-learn, with the following signature:\n",
    "```python\n",
    "\n",
    "class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40816326530612246"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (2/7)**2 - (5/7)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (1/3)**2 - (2/3)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.419047619047619"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.3*(1 - (1/3)**2 - (2/3)**2) + 0.7*(1 - (2/7)**2 - (5/7)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, given the \"measure of disorder\" (Entropy or Gini Impurity) of a parent dataset and of children datasets, we can compute the **Information Gain** which corresponds of the  **measure of the decrease** of our \"measure of disorder\"  after the data split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As its name indicates, it represents the increase of knowledge when performing the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But we actually prefer to use the **Weighted Information Gain**, which takes into account the size of the subsets created.\n",
    "\n",
    "If the data before the split contained 20 items and one of the resulting splits contained 2 items, then the weighted impurity of that subset would be 2/20 * impurity.\n",
    "\n",
    "By doing so we are lowering the importance of the impurity of sets with few elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1OTY2LCK_ZF9gogmxH9Bqmn4ner_3VlZE\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reached the **base case** (or **leaf node** ðŸ‚) when **splitting on every feature results in the largest information gain**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> ðŸ“š **Resources**: \n",
    "- [What is Entropy and Information Gain](https://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain)\n",
    "- [A Simple Explanation of Entropy in Decision Trees](https://bricaud.github.io/personal-blog/entropy-in-decision-trees/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1vGvHB-oPksgqCEI78jSUPwNU9d2AGV4v\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "0.3*np.log2(0.3) + 0.7*np.log2(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### III.2.A. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Entropy** is a **measure of randomness (or unpredictability)** in the dataset.\n",
    "\n",
    "We define the **Entropy of a dataset** like this: \n",
    "![](https://chart.apis.google.com/chart?cht=tx&chl=E%3D-%5Csum_ip_i%5Clog_2p_i%20%2C)\n",
    "\n",
    "where $p_i$ corresponds to the ratios of elements of each label `i` in the set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you chart the **Binary Entropy function** (the Entropy for a random variable `X` that can take one out of 2 values (`a` and `b` for example) using the formula above for every possible value of `p(X=a)`, it looks like this: \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://i.stack.imgur.com/OUgcx.png\">\n",
    "</p>\n",
    "\n",
    "> ðŸ”¦ **Hint**: It reaches its maximum when the probability is p=1/2, meaning that $p(X=a)=0.5$ or similarly $p(X=b)=0.5$ having a 50%/50% chance of being either a or b (uncertainty is at a maximum). The entropy function is at zero minimum when probability is p=1 or p=0 with complete certainty ( $p(X=a)=1$ or $p(X=a)=0$ respectively, latter implies $p(X=b)=1$ )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### III.2.B. Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1mGOOvmwCiBTxZCMeJ2h6ja14Frh1fM0c\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each step, we can compute the Entropy of our splitted dataset by computing the **weighted mean of Entropy** of all subsets:\n",
    "\n",
    "![](https://chart.apis.google.com/chart?cht=tx&chl=E_%7B%5Crm%20split%7D%3D%5Cfrac%7BN_1%7D%7BN%7DE_1%2B%5Cfrac%7BN_2%7D%7BN%7DE_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting our dataset into two subsets \"with less randomness\" reduces our entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## III.2. Gini Impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Should you use Gini or Entropy?*\n",
    "- Your performance will almost never change whether you use Gini impurity or Entropy.\n",
    "- Entropy might be a little slower to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# II. Implementation with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.1. Coding a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T09:20:33.180039Z",
     "start_time": "2019-10-21T09:20:33.112411Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_small = X[:,:2] # We keep only 2 features for visualisation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T09:20:33.654252Z",
     "start_time": "2019-10-21T09:20:33.650191Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"X_small.shape={}\".format(X_small.shape))\n",
    "print(\"y.shape={}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T13:27:29.298238Z",
     "start_time": "2019-07-18T13:27:29.182069Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create figure to draw chart\n",
    "plt.figure(2, figsize=(10, 8))\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_small[:, 0], X_small[:, 1], c=y, cmap=plt.cm.Set1, edgecolor='k')\n",
    "\n",
    "# Format chart\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We start by importing the proper method `DecisionTreeClassifier` from `sklearn` and to instanciate our Decision Tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T09:20:37.687187Z",
     "start_time": "2019-10-21T09:20:37.683586Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier() # Per default, criterion=\"gini\"; you could specify criterion=\"entropy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then we fit our model as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T09:20:38.302937Z",
     "start_time": "2019-10-21T09:20:38.298477Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_small, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T09:20:39.360967Z",
     "start_time": "2019-10-21T09:20:39.343383Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally we can retrieve predictions based on our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T09:20:41.606752Z",
     "start_time": "2019-10-21T09:20:41.601348Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And retrieve a score (accuracy) of our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T09:20:42.205720Z",
     "start_time": "2019-10-21T09:20:42.197796Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model has an accuracy of {}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can get a better understanding of our model by plotting the decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T13:27:36.663849Z",
     "start_time": "2019-07-18T13:27:36.480478Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create figure to draw chart\n",
    "plt.figure(2, figsize=(10, 8))\n",
    "\n",
    "# We create a grid of points contained within [x_min, x_max]x[y_min, y_max] with step h=0.02\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "h = .02  # step size of the grid\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Retrieve predictions for each point of the grid\n",
    "Z_dt = dt.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_dt = Z_dt.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary (label predicted assigned to a color)\n",
    "plt.pcolormesh(xx, yy, Z_dt, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', cmap=plt.cm.Paired)\n",
    "\n",
    "# Format chart\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.2. Visualizing our tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`sklearn` even allows us to visualize the tree. By exporting it to graphviz format and opening it with another library called `graphviz` (you will need to install it by running `pip install graphviz`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T13:27:39.818668Z",
     "start_time": "2019-07-18T13:27:39.681369Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "# We load iris data again to retrieve features and classes names\n",
    "iris = load_iris()\n",
    "\n",
    "# We export the tree in graphviz format \n",
    "dot_data = export_graphviz(dt,\n",
    "                           out_file=None,\n",
    "                           feature_names=iris.feature_names[:2],  \n",
    "                           class_names=iris.target_names,  \n",
    "                           filled=True, rounded=True)\n",
    "\n",
    "# We load the tree again with graphviz library in order to display it\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.3. Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With trees, it becomes very easy to inspect and understand important features in our model decision. \n",
    "\n",
    "You just have to inspect `feature_importances_` attribute! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T09:20:48.384326Z",
     "start_time": "2019-10-21T09:20:48.378616Z"
    }
   },
   "outputs": [],
   "source": [
    "print(dt.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or if you want to make it more visual ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T13:28:02.288516Z",
     "start_time": "2019-07-18T13:28:02.145275Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "features = iris.feature_names[:2]\n",
    "importances = dt.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "num_features = len(importances)\n",
    "\n",
    "# Plot the feature importances of the tree\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(num_features), importances[indices], color=\"g\", align=\"center\")\n",
    "plt.xticks(range(num_features), [features[i] for i in indices], rotation='45')\n",
    "plt.xlim([-1, num_features])\n",
    "plt.show()\n",
    "\n",
    "# Print values\n",
    "for i in indices:\n",
    "    print (\"{0} - {1:.3f}\".format(features[i], importances[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## II.4. Impact and meaning of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **criterion**: The function to measure the quality of a split. For example `Gini Impurity` or `Entropy` \n",
    "\n",
    "\n",
    "- **max_depth**: The maximum depth of the tree\n",
    "\n",
    "\n",
    "- **min_samples_split**: The minimum number of samples required to split an internal node\n",
    "\n",
    "\n",
    "- **min_samples_leaf**: The minimum number of samples required to be at a leaf node.\n",
    "\n",
    " \n",
    "- **min_weight_fraction_leaf** : The minimum weighted fraction of the sum total of weights (of all\n",
    "    the input samples) required to be at a leaf node. Samples have\n",
    "    equal weight when sample_weight is not provided.\n",
    "    \n",
    "\n",
    "- **max_features**: The number of features to consider when looking for the best split.\n",
    "\n",
    "\n",
    "    Note: the search for a split does not stop until at least one\n",
    "    valid partition of the node samples is found, even if it requires to\n",
    "    effectively inspect more than ``max_features`` features.\n",
    "    \n",
    "\n",
    "- **max_leaf_nodes** : Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
    "    Best nodes are defined as relative reduction in impurity.\n",
    "    If None then unlimited number of leaf nodes.\n",
    "    \n",
    "\n",
    "- **min_impurity_decrease**: A node will be split if this split induces a decrease of the impurity\n",
    "    greater than or equal to this value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
